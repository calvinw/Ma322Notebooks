{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Completion\n",
        "\n",
        "## Put Your Together AI key here"
      ],
      "id": "1abaf8c7-3b1c-42fe-b734-f1bea1361f72"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TOGETHER_API_KEY\"] = \"paste_your_api_key_here\""
      ],
      "id": "b5573626"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Packages"
      ],
      "id": "335a1f8d-6730-4ca3-be14-e8c364da4d90"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install langchain langchain_openai"
      ],
      "id": "fb188734"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the LLM"
      ],
      "id": "da395bd5-2579-4b8e-bffa-1dd0a1d327c7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#model_name = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT\"\n",
        "#model_name = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
        "#model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "#model_name=\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "#model_name=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "model_name = \"openchat/openchat-3.5-1210\"\n",
        "#model_name = \"Qwen/Qwen1.5-72B-Chat\"\n",
        "#model_name = \"cognitivecomputations/dolphin-2.5-mixtral-8x7b\"\n",
        "#model_name=\"databricks/dbrx-instruct\"\n",
        "#model_name=\"mistralai/Mixtral-8x22B\"\n",
        "\n",
        "print(\"Provider: TogetherAI\")\n",
        "print(\"Model: \" + model_name)\n",
        "\n",
        "llm=ChatOpenAI(model_name=model_name,\n",
        "               openai_api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
        "               openai_api_base=\"https://api.together.xyz/v1/\")\n",
        "\n",
        "import textwrap\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "def wrap_text(text, max_width=80):\n",
        "    \"\"\"\n",
        "    Wraps the text to the specified max_width, preserving line breaks and formatting.\n",
        "    \"\"\"\n",
        "    text = text.lstrip()\n",
        "    lines = text.splitlines()  # Split the text into lines\n",
        "    wrapped_lines = []\n",
        "    for line in lines:\n",
        "        if line.strip():  # Skip empty lines\n",
        "            wrapped_line = textwrap.fill(line, max_width, initial_indent='', subsequent_indent='')\n",
        "            wrapped_lines.extend(wrapped_line.splitlines())  # Preserve line breaks\n",
        "        else:\n",
        "            wrapped_lines.append('')  # Keep empty lines\n",
        "    return '\\n'.join(wrapped_lines)\n",
        "\n",
        "def print_prompt_and_response(prompt, response):\n",
        "    print(\"Prompt: \")\n",
        "    print(wrap_text(prompt))\n",
        "    print(\"\")\n",
        "    print(\"Response: \")\n",
        "    print(response)\n",
        "\n",
        "def print_messages_and_response(messages, response):\n",
        "    prompt = ChatPromptTemplate(messages=messages)\n",
        "    print_prompt_and_response(prompt.format(), response)\n",
        "\n",
        "def get_completion(prompt, temperature=0.0):\n",
        "    response = llm.invoke(prompt, temperature=temperature)\n",
        "    wrapped_response = wrap_text(response.content)\n",
        "    return wrapped_response\n",
        "\n",
        "def get_completion_messages(messages, temperature=0.0):\n",
        "    response=llm.invoke(messages, temperature=temperature)\n",
        "    wrapped_response = wrap_text(response.content)\n",
        "    return wrapped_response\n"
      ],
      "id": "f401962e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling an LLM with `get_completion`\n",
        "\n",
        "First set up a prompt we can use:"
      ],
      "id": "694e45e4-b6a3-4610-a528-4d79c0c01129"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Why is the sky blue? In 50 words or less.\""
      ],
      "id": "e78deeb2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we call the`get_completion` function. This function takes our prompt\n",
        "as argument and sends it off to the LLM. The LLM returns a response.\n",
        "This function returns that response."
      ],
      "id": "e2af5a64-1b74-47f2-83a1-c7358afac806"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "id": "0333eb37"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    }
  }
}