{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# More Roles"
      ],
      "id": "87a01aed-3215-40a1-9eb6-867050110a09"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name=\"openai/gpt-3.5-turbo\""
      ],
      "id": "7798075c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Put Your OPENROUTER_API_KEY here"
      ],
      "id": "aee465b7-56bf-46b3-9808-ed8479c57ab7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = \"paste_your_api_key_here\""
      ],
      "id": "223d0983"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install langchain langchain_openai"
      ],
      "id": "41ed689b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "try:\n",
        "    model_name\n",
        "except NameError:\n",
        "    model_name=\"openai/gpt-4o-mini\"\n",
        "\n",
        "print(\"Model Name:\", model_name)\n",
        "print(\"Provider:\", \"OpenRouter AI\")\n",
        "\n",
        "llm=ChatOpenAI(model_name=model_name,\n",
        "               openai_api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n",
        "               openai_api_base=\"https://openrouter.ai/api/v1\")"
      ],
      "id": "3e73adf7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import textwrap\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "from ipywidgets import widgets, Layout\n",
        "\n",
        "conversation_output = widgets.Output()\n",
        "messages = []\n",
        "\n",
        "def run_chatbot(system_prompt, initial_message):\n",
        "    global messages \n",
        "    messages = [ {'role':'system', 'content': system_prompt} ]\n",
        "    conversation_output.clear_output()\n",
        "\n",
        "    messages.append({'role': 'assistant', 'content': initial_message})\n",
        "\n",
        "    text_input = widgets.Text(\n",
        "        placeholder='Type your message here...',\n",
        "        layout=widgets.Layout(width='50%')\n",
        "    )\n",
        "    submit_button = widgets.Button(description=\"Send\")\n",
        "\n",
        "    input_box = widgets.HBox([text_input, submit_button])\n",
        "    display(conversation_output, input_box)\n",
        "\n",
        "    def on_submit_click(b):\n",
        "        message = text_input.value\n",
        "        text_input.value = ''  # Clear the input field\n",
        "\n",
        "        with conversation_output:\n",
        "            display(Markdown(f\"**User**: {message}\"))\n",
        "            messages.append({'role': 'user', 'content': message})\n",
        "            response = get_completion_messages(messages)\n",
        "            display(Markdown(f\"**AI**: {response}\"))\n",
        "            messages.append({'role': 'assistant', 'content': response})\n",
        "\n",
        "    submit_button.on_click(on_submit_click)\n",
        "\n",
        "    # Display initial AI message\n",
        "    with conversation_output:\n",
        "        display(Markdown(f\"**AI**: {initial_message}\"))\n",
        "\n",
        "def wrap_text(text, max_width=80):\n",
        "    \"\"\"\n",
        "    Wraps the text to the specified max_width, preserving line breaks and formatting.\n",
        "    \"\"\"\n",
        "    text = text.lstrip()\n",
        "    lines = text.splitlines()  # Split the text into lines\n",
        "    wrapped_lines = []\n",
        "    for line in lines:\n",
        "        if line.strip():  # Skip empty lines\n",
        "            wrapped_line = textwrap.fill(line, max_width, initial_indent='', subsequent_indent='')\n",
        "            wrapped_lines.extend(wrapped_line.splitlines())  # Preserve line breaks\n",
        "        else:\n",
        "            wrapped_lines.append('')  # Keep empty lines\n",
        "    return '\\n'.join(wrapped_lines)\n",
        "\n",
        "def print_messages():\n",
        "    for message in messages:\n",
        "        role = message['role']\n",
        "        content = message['content']\n",
        "        \n",
        "        if role == 'system':\n",
        "            print(\"System:\")\n",
        "            print(\"-\" * 40)\n",
        "            print(content)\n",
        "        elif role == 'user':\n",
        "            print(\"User: \", end=\"\")\n",
        "            print(wrap_text(content))\n",
        "        elif role == 'assistant':\n",
        "            print(\"Assistant: \", end=\"\")\n",
        "            print(wrap_text(content))\n",
        "        print()  # Add an extra newline for spacing\n",
        "\n",
        "def print_prompt_and_response(prompt, response):\n",
        "    print(\"Prompt: \")\n",
        "    print(wrap_text(prompt))\n",
        "    print(\"\")\n",
        "    print(\"Response: \")\n",
        "    print(response)\n",
        "\n",
        "def print_messages_and_response(messages, response):\n",
        "    prompt = ChatPromptTemplate(messages=messages)\n",
        "    print_prompt_and_response(prompt.format(), response)\n",
        "\n",
        "def get_completion(prompt, temperature=0.0):\n",
        "    response = llm.invoke(prompt, temperature=temperature)\n",
        "    wrapped_response = wrap_text(response.content)\n",
        "    return wrapped_response\n",
        "\n",
        "def get_completion_messages(messages, temperature=0.0):\n",
        "    response=llm.invoke(messages, temperature=temperature)\n",
        "    wrapped_response = wrap_text(response.content)\n",
        "    return wrapped_response"
      ],
      "id": "7aa7c9bd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is a Black Hole?\n",
        "\n",
        "First like a physicist talking to their collegues\n",
        "\n",
        "**Problem**:\n",
        "\n",
        "-   Tell the LLM to adopt the role or persona of a physicist talking to\n",
        "    other physicists.\n",
        "-   Ask the LLM to explain the concept of a black hole"
      ],
      "id": "756326dd-8e83-4a04-98a8-aa8146556748"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "CODE\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "id": "4a72de36"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now like a school teacher talking to some sixth grade students\n",
        "\n",
        "**Problem**:\n",
        "\n",
        "-   Tell the LLM to adopt the role of a teacher talking to sixth grade\n",
        "    students.\n",
        "-   Ask the LLM to explain the concept of a black hole"
      ],
      "id": "158615de-e7d7-4804-9bc9-5c899ee250a5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "CODE\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "id": "b03a280c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How many key strokes to type the numbers 1 to 500?\n",
        "\n",
        "So this is a problem where LLM has to realize how many digits are in the\n",
        "numbers as it tries to figure this out.\n",
        "\n",
        "-   1 through 9 are just one digit, so one keystroke for them\n",
        "-   10 through 99 are two digits, so two keystrokes for them\n",
        "-   100 through 500 are three digits, so three keystrokes for them\n",
        "\n",
        "There are 9 different numbers with 1 digit, 90 different numbers with 2\n",
        "digits, and 401 different numbers with 3 digits so:\n",
        "\n",
        "$$ 9(1) + 90(2) + 401(3) = 1392 $$\n",
        "\n",
        "Lets ask the LLM first without any role.\n",
        "\n",
        "**Problem**:\n",
        "\n",
        "-   Ask the LLM how many key strokes are needed to type the numbers from\n",
        "    1 to 500?"
      ],
      "id": "133b6046-3857-4233-9f69-f40257b20ab1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "CODE\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "id": "a9e182a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Problem**:\n",
        "\n",
        "-   Ask the LLM how many key strokes are needed to type the numbers from\n",
        "    1 to 500\n",
        "-   Tell the LLM it is an expert at counting keystrokes and it can\n",
        "    reason about it very well (try different variations too)"
      ],
      "id": "3f3a6e18-60fa-49dd-b58b-12f35201624b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "CODE\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "id": "cefb3116"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this last one you might try various wordings, the LLM we are using\n",
        "currently seems to respond slightly differently depending on how you\n",
        "word things.\n",
        "\n",
        "You may not get it to get the exact answer but often the answer is much\n",
        "better than its initial attempt.\n",
        "\n",
        "The answer still has some inaccuracies. In one version of an answer the\n",
        "LLM claimed:\n",
        "\n",
        "> Numbers from 100 to 500 require three keystokes, (the first digit, the\n",
        "> second digit and the letter “h” for hundreds)\n",
        "\n",
        "What? The letter “h” is not something that you type for a number with 3\n",
        "digits. Not sure why it counted that."
      ],
      "id": "cc0dfa32-4d6e-47f1-8325-9ed803e199e0"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    }
  }
}