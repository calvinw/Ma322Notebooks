---
title: "LLMS"
---

{{< include _llm_openai.qmd >}}

## Calling an LLM

We can call the LLM by using the `get_completion` function. This takes our
prompt and delivers it to the LLM. The LLM returns a response. We can print the
response after we get it.

```{python}
#Set up a prompt
prompt = "Why is the sky blue? In 50 words or less."

#Call the LLM
response = get_completion(prompt)

#Print out the the response
print(response)
```

## Calling an LLM with the `temperature`

Temperature is one of the settings that most LLMs have. You can set the temperature from 0.0 up to some maximum (like 1.0). Sometimes you can go higher than 1.0.

Roughly

1. Low temperature (eg 0.2-0.5)
   - Generates more predictable and conservative text

2. Medium temperature (eg. 0.6-0.8)
   - Balances between creativity and coherence

3. High temperature (eg. 0.9-1.2)
   - Generates more creative diverse, and unpredictable text

### Calling with `temperature=0.0`

Here we call the LLM 3 times with temperature 0.0

```{python}
prompt = "Why is the sky blue? In 50 words or less"
response = get_completion(prompt, temperature = 0.0)
print(response)
```

```{python}
response = get_completion(prompt, temperature = 0.0)
print(response)
```

```{python}
response = get_completion(prompt, temperature = 0.0)
print(response)
```

### Calling with `temperature=0.4`

Here we call the LLM 3 times with temperature 0.4

```{python}
response = get_completion(prompt, temperature = 0.4)
print(response)
```

```{python}
response = get_completion(prompt, temperature = 0.4)
print(response)
```

```{python}
response = get_completion(prompt, temperature = 0.4)
print(response)
```

### Calling with `temperature=1.0`

Here we call the LLM 3 times with temperature 1.0

```{python}
response = get_completion(prompt, temperature = 1.0)
print(response)
```

```{python}
response = get_completion(prompt, temperature = 1.0)
print(response)
```

```{python}
response = get_completion(prompt, temperature = 1.0)
print(response)
```


## Calling the LLM using a template

Remember the template for having the LLM give us the name of a company that
makes a particular thing? Let's call the LLM with that template and template
variable `Cars`:

```{python}
template = "Create a name for a company that makes {what}"

#Evaluate the template with what="Cars"
prompt=template.format(what="Cars")

#Call the LLM
response = get_completion(prompt)

#Print out the the response
print(response)
```

## Change the template variable `what`

Let's call the LLM using `Books` as the template variable.

```{python}
#Evaluate the template with what="Books"
prompt=template.format(what="Books")
response = get_completion(prompt)

print(response)
```

## Add another template variable for `num`

```{python}
template = "Give {num} possible names for a company that makes {what}"

#Evaluate the template
prompt=template.format(num=3, what="Cars")
response = get_completion(prompt)

print(response)
```

### Formatting the response from the LLM

Lets see if we can introduce some formatting to the output:

```{python}
template ="""Give {num} possible names for a company that makes {what}. Give
your answer in {format}"""

#Evaluate the template
prompt=template.format(num=3, what="Cars", format="A bulleted list using upper case")
response = get_completion(prompt)

print(response)
```

## Calling the LLM multiple times with our template

The following is an example of calling the LLM multiple times, each time with a
different prompt that is created from our template.

```{python}
template ="""Give {num} possible names for a company that makes {what}. Give
your answer in {format}"""

myList = ["cars", "watches", "firecrackers"]

for item in myList:
  prompt = template.format(num=2, what=item, format="upper case")
  response = get_completion(prompt)
  print_prompt_and_response(prompt, response)
```
