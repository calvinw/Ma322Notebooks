---
title: "LLMS"
---

```{python}
#| eval: true 
model_name="openai/gpt-4o-mini"
```
{{< include _llm_openrouter.qmd >}}

## Calling an LLM with `get_completion`

First set up a prompt we can use:

```{python}
prompt = "Why is the sky blue? In 50 words or less."
```

Now we call the`get_completion` function. This function takes our prompt as
argument and sends it off to the LLM. The LLM returns a response. This function
returns that response.

```{python}
response = get_completion(prompt)
print(response)
```

Sometimes its nice to print out the prompt and the response when we are working
like this. We can use the `print` function for this: 

```{python}
response = get_completion(prompt)
print("Prompt:")
print(prompt)
print("Response:")
print(response)
```

## Calling an LLM with the `temperature`

Temperature is one of the settings that most LLMs have. You can set the
temperature from 0.0 up to some maximum (like 1.0 or 1.2). Sometimes you can go
higher.

Roughly

1. Low temperature (eg 0.2-0.5)
   - Generates more predictable and conservative text

2. Medium temperature (eg. 0.6-0.8)
   - Balances between creativity and coherence

3. High temperature (eg. 0.9-1.2)
   - Generates more creative diverse, and unpredictable text

First lets set up a prompt to call several times with different temperatures.

```{python}
prompt = "Why is the sky blue? In 50 words or less"
```

### Calling with `temperature=0.0`

`temperature` is parameter you can pass to the `get_completion` function. You
see this in the code below.

Here we call the LLM 3 times with temperature 0.0

```{python}
response = get_completion(prompt, temperature = 0.0)
print(response)
print("------")

response = get_completion(prompt, temperature = 0.0)
print(response)
print("------")

response = get_completion(prompt, temperature = 0.0)
print(response)
print("------")
```

### Calling with `temperature=0.4`

Here we call the LLM 3 times with temperature 0.4

```{python}
response = get_completion(prompt, temperature = 0.4)
print(response)
print("------")

response = get_completion(prompt, temperature = 0.4)
print(response)
print("------")

response = get_completion(prompt, temperature = 0.4)
print(response)
print("------")
```


### Calling with `temperature=1.0`


```{python}
response = get_completion(prompt, temperature = 1.0)
print(response)
print("------")

response = get_completion(prompt, temperature = 1.0)
print(response)
print("------")

response = get_completion(prompt, temperature = 1.0)
print(response)
print("------")
```

## Calling the LLM using a template

Remember the template for having the LLM give us the name of a company that
makes a particular thing?

```{python}
template = "Recommend a name for a company that makes {what}"
```
Lets set that up and call it with `what="cars"`:

```{python}
prompt=template.format(what="cars")
response = get_completion(prompt)
print("Prompt:")
print(prompt)
print("Response:")
print(response)
```

## Change the template variable `{what}`

Let's call the LLM using `what="books"` as the template variable.

```{python}
prompt=template.format(what="books")
response = get_completion(prompt)
print("Prompt:")
print(prompt)
print("Response:")
print(response)
```

## Add A Template Variable `{num}`

```{python}
template = "Recommend {num} names for a company that makes {what}"
```

Lets call it with `num=3` and `what="cars"`:

```{python}
prompt=template.format(num=3, what="cars")
response = get_completion(prompt)
print("Prompt:")
print(prompt)
print("Response:")
print(response)
```

### Add a Template Variable `{format}`

Lets see if we can introduce some formatting to the output:

```{python}
template ="""Recommend {num} names for a company that makes {what}.
Give your answer in {format}"""
```

Call the LLM:

```{python}
prompt=template.format(num=3, what="cars", format="A bulleted list using upper case")
response = get_completion(prompt)
print("Prompt:")
print(prompt)
print("Response:")
print(response)
```
