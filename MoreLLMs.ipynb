{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# More LLMS"
      ],
      "id": "2e0f5638-8997-45f6-a7c1-bab0ef5e8deb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.environ.get(\"TOGETHER_API_KEY\") is None:\n",
        "   os.environ[\"TOGETHER_API_KEY\"] = \"paste_your_api_key_here\""
      ],
      "id": "25f6526c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup the LLM"
      ],
      "id": "d9dfbeed-4393-4ba4-bc80-bfee48f01d10"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install langchain langchain_openai"
      ],
      "id": "43fdcd3b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model_name = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT\"\n",
        "#model_name = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
        "#model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "#model_name=\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "#model_name=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "#model_name = \"openchat/openchat-3.5-1210\"\n",
        "#model_name = \"Qwen/Qwen1.5-72B-Chat\"\n",
        "\n",
        "print(\"Provider: TogetherAI\")\n",
        "print(\"Model: \" + model_name)\n",
        "\n",
        "llm=ChatOpenAI(model_name=model_name,\n",
        "               openai_api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
        "               openai_api_base=\"https://api.together.xyz/v1/\")"
      ],
      "id": "0212b60e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import textwrap\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "def wrap_text(text, max_width=80):\n",
        "    \"\"\"\n",
        "    Wraps the text to the specified max_width, preserving line breaks and formatting.\n",
        "    \"\"\"\n",
        "    text = text.lstrip()\n",
        "    lines = text.splitlines()  # Split the text into lines\n",
        "    wrapped_lines = []\n",
        "    for line in lines:\n",
        "        if line.strip():  # Skip empty lines\n",
        "            wrapped_line = textwrap.fill(line, max_width, initial_indent='', subsequent_indent='')\n",
        "            wrapped_lines.extend(wrapped_line.splitlines())  # Preserve line breaks\n",
        "        else:\n",
        "            wrapped_lines.append('')  # Keep empty lines\n",
        "    return '\\n'.join(wrapped_lines)\n",
        "\n",
        "def print_prompt_and_response(prompt, response):\n",
        "    print(\"Prompt: \")\n",
        "    print(wrap_text(prompt))\n",
        "    print(\"\")\n",
        "    print(\"Response: \")\n",
        "    print(response)\n",
        "\n",
        "def print_messages_and_response(messages, response):\n",
        "    prompt = ChatPromptTemplate(messages=messages)\n",
        "    print_prompt_and_response(prompt.format(), response)\n",
        "\n",
        "def get_completion(prompt, temperature=0.0):\n",
        "    response = llm.invoke(prompt, temperature=temperature)\n",
        "    wrapped_response = wrap_text(response.content)\n",
        "    return wrapped_response\n",
        "\n",
        "def get_completion_messages(messages, temperature=0.0):\n",
        "    response=llm.invoke(messages, temperature=temperature)\n",
        "    wrapped_response = wrap_text(response.content)\n",
        "    return wrapped_response"
      ],
      "id": "9b05fc26"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling an LLM\n",
        "\n",
        "-   Choose a question and set up a prompt to send the question to the\n",
        "    LLM. Any question you like."
      ],
      "id": "9bedac50-1496-4c46-b54e-e40435abd2d7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = CODE"
      ],
      "id": "e5fda6d6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Run the cell below to print out your response"
      ],
      "id": "a1532d27-c931-41f2-a516-c3e0df68d50c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "id": "f370bf95"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Run the cell below to print out your prompt and your response\n",
        "    together"
      ],
      "id": "3b38ef65-03b3-4b35-a860-ff4dd70cf9ab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt)\n",
        "print_prompt_and_response(prompt,response)"
      ],
      "id": "57e75995"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling an LLM with `temperature=0.0`\n",
        "\n",
        "Call the LLM 3 times with your question above, using temperature 0.0 for\n",
        "these.\n",
        "\n",
        "-   Using the same prompt as above call `get_completion` three times\n",
        "    with `temperature=0`\n",
        "-   Each time print out the response you get"
      ],
      "id": "4a5b9be2-ae83-4166-a3f4-b27f29e21df1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CODE"
      ],
      "id": "606467a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Exactly the same as above but use `temperature=0.7` instead"
      ],
      "id": "7bfc5880-a2b9-4ae1-9730-c8cfeb0a6775"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CODE"
      ],
      "id": "615acfc0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Template For Asking For a Capital\n",
        "\n",
        "-   Create a template string with using a template variable `{place}`\n",
        "    for asking the LLM to give you the capital of `{place}`."
      ],
      "id": "f3e3b99d-cbfc-4513-b578-03db24b91cbd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = CODE"
      ],
      "id": "0ab936d2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Evaluate your template with `place=\"Texas\"`\n",
        "-   Call `get_completion` to get the response from the LLM\n",
        "-   print the response using `print_prompt_and_response`"
      ],
      "id": "7030c612-b795-42cd-bbd0-d1743b0928d1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt=template.format(CODE)\n",
        "\n",
        "# Call the LLM by passing the prompt to get_completion\n",
        "response=get_completion(CODE)\n",
        "\n",
        "# Print the response\n",
        "print_prompt_and_response(prompt,response)"
      ],
      "id": "6bdc2248"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding a Variable Called `num`\n",
        "\n",
        "-   Add another template variable called `{num}` to the template and ask\n",
        "    the LLM to provide you with `num` facts about the capital that you\n",
        "    specified with `{place}`."
      ],
      "id": "3728038c-01cc-49d3-8ae1-8c3d44bab7b5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = CODE"
      ],
      "id": "7f01d6a5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Evaluate your template with `place=\"Texas\"` and `num=3` and save it\n",
        "    in the `prompt` variable"
      ],
      "id": "b8359380-0ae2-47f2-9edb-e750d9a0a4a8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt=template.format(CODE)"
      ],
      "id": "95b927f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Now call `get_completion` with the prompt and get the response.\n",
        "    Store it in the `response` variable"
      ],
      "id": "c73967f3-4ea0-42c5-b248-c303da12ad1c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(CODE)"
      ],
      "id": "1f9d875a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Use `print_prompt_and_response` to print out the prompt and\n",
        "    completion."
      ],
      "id": "e5f8b922-938c-478f-a20b-ad2b624470ad"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CODE"
      ],
      "id": "6084e7b5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ask To Format the Response\n",
        "\n",
        "-   Add a template variable called `format` so you can pass in the\n",
        "    desired format for the facts that are given.\n",
        "-   Call your template with “Texas” and 3 facts and “uppercase” for the\n",
        "    format"
      ],
      "id": "8ac60649-502b-49cf-a1de-60414ea4aa8c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define template here\n",
        "template = CODE\n",
        "\n",
        "# Evaluate the template using place=\"Texas\" and num=3 and format=\"upper case\"\n",
        "prompt=template.format(CODE)\n",
        "\n",
        "# Call get_completion and get the response, then print it.\n",
        "response = get_completion(prompt)\n",
        "print_prompt_and_response(prompt,response)"
      ],
      "id": "4d08144c"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    }
  }
}