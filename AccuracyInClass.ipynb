{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Accuracy"
      ],
      "id": "4193dc58-32cd-4cf3-a045-2817d306c2aa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "results": "hide"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "id": "6bdcf95e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating Models\n",
        "\n",
        "### Model 1\n",
        "\n",
        "Suppose we have created a model 1 and made some predictions.\n",
        "\n",
        "Here are the actuals from some data and predicted for that same data:\n",
        "\n",
        "#### Model 1 predictions vs actuals\n",
        "\n",
        "| actuals | predicted |\n",
        "|---------|-----------|\n",
        "| yes     | yes       |\n",
        "| yes     | no        |\n",
        "| yes     | yes       |\n",
        "| yes     | yes       |\n",
        "| no      | yes       |\n",
        "| no      | no        |\n",
        "| no      | yes       |\n",
        "| yes     | no        |\n",
        "| no      | yes       |\n",
        "| yes     | yes       |\n",
        "| yes     | yes       |\n",
        "\n",
        "-   create two lists called `actuals` and `predicted` with the data\n",
        "    above.\n",
        "-   create a DataFrame called `df` that has these columns\n",
        "-   print out the DataFrame with `print(df)`"
      ],
      "id": "d89935c8-16f6-4320-9732-87bb27e3d0f0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "actuals = # CODE\n",
        "predicted = # CODE\n",
        "df = pd.DataFrame({'actuals': actuals, 'predicted': predicted})\n",
        "print(df)"
      ],
      "id": "ff209805"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "actuals = ['yes','yes','yes','yes','no','no','no','yes','no','yes', 'yes']\n",
        "predicted = ['yes','no','yes','yes','yes','no','yes','no','yes','yes', 'yes']\n",
        "df = pd.DataFrame({'actuals': actuals, 'predicted': predicted})\n",
        "print(df)"
      ],
      "id": "b3e35681"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 1 Accuracy\n",
        "\n",
        "-   Find the accuracy by using `accuracy_score(actuals, predicted)`"
      ],
      "id": "d6f15157-b7c6-495c-8e6e-ab5e6ad6ed8a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CODE"
      ],
      "id": "e0f9577a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(accuracy_score(actuals, predicted))"
      ],
      "id": "9bf8c7c2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accuracy is ???\n",
        "\n",
        "### Model 1 Confusion Matrix\n",
        "\n",
        "Remember:\n",
        "\n",
        "|            |            | **Predicted** |            |\n",
        "|------------|------------|---------------|------------|\n",
        "|            |            | *Negative*    | *Positive* |\n",
        "| **Actual** | *Negative* | TN            | FP         |\n",
        "|            | *Positive* | FN            | TP         |\n",
        "\n",
        "-   Find the confusion matrix using\n",
        "    `confusion_matrix(actuals, predicted)`"
      ],
      "id": "b5d26f7e-5f86-40f3-80b7-e3677bee45a4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CODE"
      ],
      "id": "2aeb7c93"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(confusion_matrix(actuals, predicted, labels=['no', 'yes']))"
      ],
      "id": "2729ba41"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   $TP$ is ???\n",
        "-   $FP$ is ???\n",
        "-   $TN$ is ???\n",
        "-   $FN$ is ???\n",
        "\n",
        "### Model 2\n",
        "\n",
        "#### Model 2 predictions vs actuals\n",
        "\n",
        "| actuals | predicted |\n",
        "|---------|-----------|\n",
        "| yes     | no        |\n",
        "| yes     | no        |\n",
        "| yes     | yes       |\n",
        "| yes     | no        |\n",
        "| no      | no        |\n",
        "| no      | yes       |\n",
        "| no      | no        |\n",
        "| yes     | yes       |\n",
        "| no      | no        |\n",
        "| yes     | yes       |\n",
        "| yes     | yes       |\n",
        "\n",
        "-   create two lists called `actuals` and `predicted` with the data\n",
        "    above.\n",
        "-   create a DataFrame called `df` that has these columns\n",
        "-   print out the DataFrame with `print(df)`"
      ],
      "id": "f64e2c79-3443-44d9-a606-fad035f41fb3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CODE"
      ],
      "id": "44f7b697"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "actuals = ['yes','yes','yes','yes','no','no','no','yes','no','yes','yes']\n",
        "predicted = ['no','no','yes','no','no','yes','no','yes','no','yes','yes']\n",
        "df = pd.DataFrame({'actuals': actuals, 'predicted': predicted})\n",
        "print(df)"
      ],
      "id": "5cc0183f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 2 Accuracy\n",
        "\n",
        "-   Find the accuracy by using `accuracy_score(actuals, predicted)`"
      ],
      "id": "cc315a21-593c-4763-953b-d682453d7375"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CODE"
      ],
      "id": "ad5c87b8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(accuracy_score(actuals, predicted))"
      ],
      "id": "0fc58dda"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accuracy is ???\n",
        "\n",
        "### Model 2 Confusion Matrix\n",
        "\n",
        "Remember:\n",
        "\n",
        "|            |            | **Predicted** |            |\n",
        "|------------|------------|---------------|------------|\n",
        "|            |            | *Negative*    | *Positive* |\n",
        "| **Actual** | *Negative* | TN            | FP         |\n",
        "|            | *Positive* | FN            | TP         |\n",
        "\n",
        "-   Find the confusion matrix using\n",
        "    `confusion_matrix(actuals, predicted)`"
      ],
      "id": "0117bbb5-f6c1-4488-bdf8-33ccf923a984"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CODE"
      ],
      "id": "c803bff7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(confusion_matrix(actuals, predicted, labels=['no', 'yes']))"
      ],
      "id": "f93609aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   $TP$ is ???\n",
        "-   $FP$ is ???\n",
        "-   $TN$ is ???\n",
        "-   $FN$ is ???\n",
        "\n",
        "## Model 1 or Model 2\n",
        "\n",
        "Which model (Model 1 or Model 2) had the highest accuracy: ???\n",
        "\n",
        "Which model had the most false positives? ???\n",
        "\n",
        "Which model had the most false negatives? ???"
      ],
      "id": "655ae876-4032-4f37-b616-bafe7163cdbf"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    }
  }
}