{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Roles\n",
        "\n",
        "## Put Your Together AI key here"
      ],
      "id": "bf792592-cf68-434d-b188-b754b64a47d1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TOGETHER_API_KEY\"] = \"paste_your_api_key_here\""
      ],
      "id": "0d763b05"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Packages"
      ],
      "id": "0016abcb-ef0b-43bf-a735-2c9fc4f78c63"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install langchain langchain_openai"
      ],
      "id": "1fd02d39"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the LLM"
      ],
      "id": "30b71b85-564a-4d0d-b272-b23577c02ff8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#model_name = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT\"\n",
        "#model_name = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
        "#model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "#model_name=\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "#model_name=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "model_name = \"openchat/openchat-3.5-1210\"\n",
        "#model_name = \"Qwen/Qwen1.5-72B-Chat\"\n",
        "#model_name = \"cognitivecomputations/dolphin-2.5-mixtral-8x7b\"\n",
        "#model_name=\"databricks/dbrx-instruct\"\n",
        "#model_name=\"mistralai/Mixtral-8x22B\"\n",
        "\n",
        "print(\"Provider: TogetherAI\")\n",
        "print(\"Model: \" + model_name)\n",
        "\n",
        "llm=ChatOpenAI(model_name=model_name,\n",
        "               openai_api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
        "               openai_api_base=\"https://api.together.xyz/v1/\")\n",
        "\n",
        "import textwrap\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "def wrap_text(text, max_width=80):\n",
        "    \"\"\"\n",
        "    Wraps the text to the specified max_width, preserving line breaks and formatting.\n",
        "    \"\"\"\n",
        "    text = text.lstrip()\n",
        "    lines = text.splitlines()  # Split the text into lines\n",
        "    wrapped_lines = []\n",
        "    for line in lines:\n",
        "        if line.strip():  # Skip empty lines\n",
        "            wrapped_line = textwrap.fill(line, max_width, initial_indent='', subsequent_indent='')\n",
        "            wrapped_lines.extend(wrapped_line.splitlines())  # Preserve line breaks\n",
        "        else:\n",
        "            wrapped_lines.append('')  # Keep empty lines\n",
        "    return '\\n'.join(wrapped_lines)\n",
        "\n",
        "def print_prompt_and_response(prompt, response):\n",
        "    print(\"Prompt: \")\n",
        "    print(wrap_text(prompt))\n",
        "    print(\"\")\n",
        "    print(\"Response: \")\n",
        "    print(response)\n",
        "\n",
        "def print_messages_and_response(messages, response):\n",
        "    prompt = ChatPromptTemplate(messages=messages)\n",
        "    print_prompt_and_response(prompt.format(), response)\n",
        "\n",
        "def get_completion(prompt, temperature=0.0):\n",
        "    response = llm.invoke(prompt, temperature=temperature)\n",
        "    wrapped_response = wrap_text(response.content)\n",
        "    return wrapped_response\n",
        "\n",
        "def get_completion_messages(messages, temperature=0.0):\n",
        "    response=llm.invoke(messages, temperature=temperature)\n",
        "    wrapped_response = wrap_text(response.content)\n",
        "    return wrapped_response\n"
      ],
      "id": "5127669f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Many times it helps to give the LLM a role or a persona. That helps set\n",
        "the context for how it is meant to respond. This is a very common thing\n",
        "to do and is why you see many prompts that start with:\n",
        "\n",
        "-   You are an expert at …\n",
        "-   You are a professional …\n",
        "\n",
        "This instructions ground the LLMs responses with information about how\n",
        "it should act and respond.\n",
        "\n",
        "## Act Like a Mouse\n",
        "\n",
        "We can tell the LLM that it is to act like a mouse."
      ],
      "id": "e1b65867-5013-4bef-bf9b-189d898c5fd1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt=\"\"\"\n",
        "You are a mouse.\n",
        "What do you think of skateboards?\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "id": "370aa38a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Act Like a Kid"
      ],
      "id": "b4f97d22-0953-44d7-8868-beafd326c48f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt=\"\"\"\n",
        "You are a kid.\n",
        "What do you think of skateboards?\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "id": "3b503dc2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Is a Married Person Looking at an Unmarried Person?\n",
        "\n",
        "In this part we ask the LLM to solve a logical problem. We will ask it\n",
        "to solve this problem twice. First without prompting it with a role.\n",
        "Then the second time we will ask it to answer by giving it a role and\n",
        "asking it to think about all the cases.\n",
        "\n",
        "So here is our question:\n",
        "\n",
        "> Jack is looking at Anne. Anne is looking at George. Jack is married,\n",
        "> George is not, and we don’t know if Anne is married. Is a married\n",
        "> person looking at an unmarried person?\n",
        "\n",
        "The answer to our question is yes. And the way to see it is to just\n",
        "think about the cases:\n",
        "\n",
        "-   If Anne is married, then Anne (married) is looking at George\n",
        "    (unmarried)\n",
        "-   If Anne is unmarried, then Jack (married) is looking at Anne\n",
        "    (unmarried)\n",
        "\n",
        "So in both cases the answer is yes, a married person is looking at an\n",
        "unmarried person.\n",
        "\n",
        "Now first we just try to have it solve the problem without any\n",
        "additional prompting about a role."
      ],
      "id": "c5d11895-a5f6-4557-8d09-75b9853d20f1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt=\"\"\"\n",
        "Jack is looking at Anne. Anne is looking at George. Jack is married, George is\n",
        "not, and we don’t know if Anne is married. Is a married person looking at an\n",
        "unmarried person?\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "id": "cd0eb3e8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The answer above is right, but the LLM thinks its true since Jack is\n",
        "looking at George. Which is not quite correct.\n",
        "\n",
        "## Act Like a Professor of Logic\n",
        "\n",
        "But now we tell the LLM its a professor of logic and it should think\n",
        "through all the possibilities. With many LLMs this gets them closer to\n",
        "the right solution and sometimes they get it completly right with this\n",
        "kind of prompting.\n",
        "\n",
        "Asking a model to think “step-by-step” or by listing its reasoning is\n",
        "called “Chain of Thought(CoT)” prompting."
      ],
      "id": "142cf106-f18a-4692-86b5-b90aa4a9a972"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt=\"\"\"\n",
        "You are a smart professor of logic who can answer complex logic problems. Think\n",
        "through all the cases before you answer.\n",
        "\n",
        "Logic Problem:\n",
        "Jack is looking at Anne. Anne is looking at George. Jack is\n",
        "married, George is not, and we don’t know if Anne is married. Is a married\n",
        "person looking at an unmarried person?\n",
        "\"\"\"\n",
        "\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "id": "5eadecab"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time the LLM does much better and you see it doing better thinking\n",
        "step-by-step.\n",
        "\n",
        "## Is This Equation Solved Correctly?\n",
        "\n",
        "Here is another example involving a simple algebra problem.\n",
        "\n",
        "Without further prompting we ask the LLM if this (incorrect) problem\n",
        "solution is correct."
      ],
      "id": "b0378f34-8335-4594-9e46-f3d7a6d7070e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt =\"\"\"\n",
        "Is this equation solved correctly below?\n",
        "\n",
        "2x-3=9\n",
        "2x=6\n",
        "x=3\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "id": "73910a9c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hmm…it seems to think the solution is correct. It didn’t see anything\n",
        "wrong with the solution.\n",
        "\n",
        "## You are a Teacher of Math. Reason Step-By-Step\n",
        "\n",
        "Now we ask it again, but this time we tell it that its a teacher of\n",
        "math."
      ],
      "id": "d74583a2-7761-4f95-8caf-916ff097df73"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "You are a teacher of math. And can reason step-by-step to see if something is\n",
        "correct. First work out your own solution to the problem. Then carefully\n",
        "consider how to answer the following question:\n",
        "\n",
        "Is this problem solved correctly below?\n",
        "2x-3=9\n",
        "2x=6\n",
        "x=3\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "id": "5a1af3b8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here it does much better in the second attempt. It still says the\n",
        "problem is solved correctly originally, but then it finds that there is\n",
        "a mistake. Definitely the step-by-step reasoning it is doing is an\n",
        "improvement over its first answer."
      ],
      "id": "c2199566-701b-4f95-9862-9497f02dc676"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    }
  }
}