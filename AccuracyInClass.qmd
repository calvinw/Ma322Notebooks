---
title: "Accuracy"
format:
  html:
    code-fold: true
jupyter: python3
---

```{python}
#| echo: true
#| results: 'hide'
#| message: false
#| warning: false

import pandas as pd
from sklearn.metrics import accuracy_score, confusion_matrix
```

## Evaluating Models

### Model 1

Suppose we have created a model 1 and made some predictions. 

Here are the actuals from some data and predicted for that same data:

#### Model 1 predictions vs actuals

| actuals | predicted |
|-|-|
| yes | yes |
| yes | no |
| yes | yes |
| yes  | yes |
| no  | yes |
| no | no |
| no | yes |
| yes | no |
| no  | yes |
| yes  | yes |
| yes  | yes |

- create two lists called `actuals` and `predicted` with the data above.
- create a DataFrame called `df` that has these columns
- print out the DataFrame with `print(df)` 
```{python}
#| eval: false
actuals = # CODE
predicted = # CODE
df = pd.DataFrame({'actuals': actuals, 'predicted': predicted})
print(df)
```
```{python}
actuals = ['yes','yes','yes','yes','no','no','no','yes','no','yes', 'yes']
predicted = ['yes','no','yes','yes','yes','no','yes','no','yes','yes', 'yes']
df = pd.DataFrame({'actuals': actuals, 'predicted': predicted})
print(df)
```

### Model 1 Accuracy

- Find the accuracy by using `accuracy_score(actuals, predicted)`
```{python}
#| eval: false
# CODE
```
```{python}
print(accuracy_score(actuals, predicted))
```

Accuracy is ???

### Model 1 Confusion Matrix 

Remember:

|           |            |**Predicted**|            |
|-----------|------------|-------------|------------|
|           |            | _Negative_  | _Positive_ |
|**Actual** | _Negative_ |     TN      |     FP     |
|           | _Positive_ |     FN      |     TP     |

- Find the confusion matrix using `confusion_matrix(actuals, predicted)`
```{python}
#| eval: false
# CODE
```
```{python}
print(confusion_matrix(actuals, predicted, labels=['no', 'yes']))
```

- $TP$  is ??? 
- $FP$  is ??? 
- $TN$  is ??? 
- $FN$  is ??? 


### Model 2 

#### Model 2 predictions vs actuals

| actuals | predicted |
|-|-|
| yes | no |
| yes | no |
| yes | yes |
| yes  | no |
| no  | no |
| no | yes |
| no | no |
| yes | yes |
| no  | no |
| yes  | yes |
| yes  | yes |


- create two lists called `actuals` and `predicted` with the data above.
- create a DataFrame called `df` that has these columns
- print out the DataFrame with `print(df)` 
```{python}
#| eval: false
# CODE
```
```{python}
actuals = ['yes','yes','yes','yes','no','no','no','yes','no','yes','yes']
predicted = ['no','no','yes','no','no','yes','no','yes','no','yes','yes']
df = pd.DataFrame({'actuals': actuals, 'predicted': predicted})
print(df)
```

### Model 2 Accuracy

- Find the accuracy by using `accuracy_score(actuals, predicted)`

```{python}
#| eval: false
# CODE
```
```{python}
print(accuracy_score(actuals, predicted))
```

Accuracy is ???

### Model 2 Confusion Matrix 

Remember:

|           |            |**Predicted**|            |
|-----------|------------|-------------|------------|
|           |            | _Negative_  | _Positive_ |
|**Actual** | _Negative_ |     TN      |     FP     |
|           | _Positive_ |     FN      |     TP     |

- Find the confusion matrix using `confusion_matrix(actuals, predicted)`

```{python}
#| eval: false
# CODE
```
```{python}
print(confusion_matrix(actuals, predicted, labels=['no', 'yes']))
```

- $TP$  is ??? 
- $FP$  is ??? 
- $TN$  is ??? 
- $FN$  is ??? 

## Model 1 or Model 2

Which model (Model 1 or Model 2) had the highest accuracy: ???

Which model had the most false positives? ???

Which model had the most false negatives? ???
