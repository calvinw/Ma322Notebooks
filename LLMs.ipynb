{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLMS"
      ],
      "id": "75a3114b-7859-4760-85cb-884ba16ecc9c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name=\"openai/gpt-4o-mini\""
      ],
      "id": "547f55df"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Put Your OPENROUTER_API_KEY here"
      ],
      "id": "14570130-76d0-4dbd-8e32-cf6f9d816e6c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = \"paste_your_api_key_here\""
      ],
      "id": "9f187f48"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install langchain langchain_openai\n",
        "!pip3 install --upgrade openai"
      ],
      "id": "fcc494e9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "try:\n",
        "    model_name\n",
        "except NameError:\n",
        "    model_name=\"openai/gpt-4o-mini\"\n",
        "\n",
        "print(\"Model Name:\", model_name)\n",
        "print(\"Provider:\", \"OpenRouter AI\")\n",
        "\n",
        "llm=ChatOpenAI(model_name=model_name,\n",
        "               openai_api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n",
        "               openai_api_base=\"https://openrouter.ai/api/v1\")"
      ],
      "id": "82329f2d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import textwrap\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "from ipywidgets import widgets, Layout\n",
        "\n",
        "conversation_output = widgets.Output()\n",
        "messages = []\n",
        "\n",
        "def run_chatbot(system_prompt, initial_message):\n",
        "    global messages \n",
        "    messages = [ {'role':'system', 'content': system_prompt} ]\n",
        "    conversation_output.clear_output()\n",
        "\n",
        "    messages.append({'role': 'assistant', 'content': initial_message})\n",
        "\n",
        "    text_input = widgets.Text(\n",
        "        placeholder='Type your message here...',\n",
        "        layout=widgets.Layout(width='50%')\n",
        "    )\n",
        "    submit_button = widgets.Button(description=\"Send\")\n",
        "\n",
        "    input_box = widgets.HBox([text_input, submit_button])\n",
        "    display(conversation_output, input_box)\n",
        "\n",
        "    def on_submit_click(b):\n",
        "        message = text_input.value\n",
        "        text_input.value = ''  # Clear the input field\n",
        "\n",
        "        with conversation_output:\n",
        "            display(Markdown(f\"**User**: {message}\"))\n",
        "            messages.append({'role': 'user', 'content': message})\n",
        "            response = get_completion_messages(messages)\n",
        "            display(Markdown(f\"**AI**: {response}\"))\n",
        "            messages.append({'role': 'assistant', 'content': response})\n",
        "\n",
        "    submit_button.on_click(on_submit_click)\n",
        "\n",
        "    # Display initial AI message\n",
        "    with conversation_output:\n",
        "        display(Markdown(f\"**AI**: {initial_message}\"))\n",
        "\n",
        "def wrap_text(text, max_width=80):\n",
        "    \"\"\"\n",
        "    Wraps the text to the specified max_width, preserving line breaks and formatting.\n",
        "    \"\"\"\n",
        "    text = text.lstrip()\n",
        "    lines = text.splitlines()  # Split the text into lines\n",
        "    wrapped_lines = []\n",
        "    for line in lines:\n",
        "        if line.strip():  # Skip empty lines\n",
        "            wrapped_line = textwrap.fill(line, max_width, initial_indent='', subsequent_indent='')\n",
        "            wrapped_lines.extend(wrapped_line.splitlines())  # Preserve line breaks\n",
        "        else:\n",
        "            wrapped_lines.append('')  # Keep empty lines\n",
        "    return '\\n'.join(wrapped_lines)\n",
        "\n",
        "def print_messages():\n",
        "    for message in messages:\n",
        "        role = message['role']\n",
        "        content = message['content']\n",
        "        \n",
        "        if role == 'system':\n",
        "            print(\"System:\")\n",
        "            print(\"-\" * 40)\n",
        "            print(content)\n",
        "        elif role == 'user':\n",
        "            print(\"User: \", end=\"\")\n",
        "            print(wrap_text(content))\n",
        "        elif role == 'assistant':\n",
        "            print(\"Assistant: \", end=\"\")\n",
        "            print(wrap_text(content))\n",
        "        print()  # Add an extra newline for spacing\n",
        "\n",
        "def print_prompt_and_response(prompt, response):\n",
        "    print(\"Prompt: \")\n",
        "    print(wrap_text(prompt))\n",
        "    print(\"\")\n",
        "    print(\"Response: \")\n",
        "    print(response)\n",
        "\n",
        "def print_messages_and_response(messages, response):\n",
        "    prompt = ChatPromptTemplate(messages=messages)\n",
        "    print_prompt_and_response(prompt.format(), response)\n",
        "\n",
        "def get_completion(prompt, temperature=0.0):\n",
        "    response = llm.invoke(prompt, temperature=temperature)\n",
        "    wrapped_response = wrap_text(response.content)\n",
        "    return wrapped_response\n",
        "\n",
        "def get_completion_messages(messages, temperature=0.0):\n",
        "    response=llm.invoke(messages, temperature=temperature)\n",
        "    wrapped_response = wrap_text(response.content)\n",
        "    return wrapped_response"
      ],
      "id": "972ecc42"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling an LLM with `get_completion`\n",
        "\n",
        "First set up a prompt we can use:"
      ],
      "id": "04ffac64-d244-4a91-a72d-b92e6bd18aab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Why is the sky blue? In 50 words or less.\""
      ],
      "id": "f51ba497"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we call the`get_completion` function. This function takes our prompt\n",
        "as argument and sends it off to the LLM. The LLM returns a response.\n",
        "This function returns that response."
      ],
      "id": "cb701348-e74d-48eb-b880-aecf06f994eb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "id": "384975d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sometimes its nice to print out the prompt and the response when we are\n",
        "working like this. We can use the `print` function for this:"
      ],
      "id": "ad3cafdf-014b-4cc6-8d37-c93ff80b7f81"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt)\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"Response:\")\n",
        "print(response)"
      ],
      "id": "3cd86351"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling an LLM with the `temperature`\n",
        "\n",
        "Temperature is one of the settings that most LLMs have. You can set the\n",
        "temperature from 0.0 up to some maximum (like 1.0 or 1.2). Sometimes you\n",
        "can go higher.\n",
        "\n",
        "Roughly\n",
        "\n",
        "1.  Low temperature (eg 0.2-0.5)\n",
        "    -   Generates more predictable and conservative text\n",
        "2.  Medium temperature (eg. 0.6-0.8)\n",
        "    -   Balances between creativity and coherence\n",
        "3.  High temperature (eg. 0.9-1.2)\n",
        "    -   Generates more creative diverse, and unpredictable text\n",
        "\n",
        "First lets set up a prompt to call several times with different\n",
        "temperatures."
      ],
      "id": "02496f9b-1cfc-4b6f-9bcd-cb54183e9bbe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Why is the sky blue? In 50 words or less\""
      ],
      "id": "8fff9560"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calling with `temperature=0.0`\n",
        "\n",
        "`temperature` is parameter you can pass to the `get_completion`\n",
        "function. You see this in the code below.\n",
        "\n",
        "Here we call the LLM 3 times with temperature 0.0"
      ],
      "id": "44812c7f-aae1-4f09-aaec-5472a9b3b8cd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt, temperature = 0.0)\n",
        "print(response)\n",
        "print(\"------\")\n",
        "\n",
        "response = get_completion(prompt, temperature = 0.0)\n",
        "print(response)\n",
        "print(\"------\")\n",
        "\n",
        "response = get_completion(prompt, temperature = 0.0)\n",
        "print(response)\n",
        "print(\"------\")"
      ],
      "id": "4bd6f908"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calling with `temperature=0.4`\n",
        "\n",
        "Here we call the LLM 3 times with temperature 0.4"
      ],
      "id": "71d9ffe5-cb1a-49f8-90f9-5a0938a1c125"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt, temperature = 0.4)\n",
        "print(response)\n",
        "print(\"------\")\n",
        "\n",
        "response = get_completion(prompt, temperature = 0.4)\n",
        "print(response)\n",
        "print(\"------\")\n",
        "\n",
        "response = get_completion(prompt, temperature = 0.4)\n",
        "print(response)\n",
        "print(\"------\")"
      ],
      "id": "31239c03"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calling with `temperature=1.0`"
      ],
      "id": "752fbe93-6ee2-4c97-956b-47168f20edba"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt, temperature = 1.0)\n",
        "print(response)\n",
        "print(\"------\")\n",
        "\n",
        "response = get_completion(prompt, temperature = 1.0)\n",
        "print(response)\n",
        "print(\"------\")\n",
        "\n",
        "response = get_completion(prompt, temperature = 1.0)\n",
        "print(response)\n",
        "print(\"------\")"
      ],
      "id": "23e6e6ab"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling the LLM using a template\n",
        "\n",
        "Remember the template for having the LLM give us the name of a company\n",
        "that makes a particular thing?"
      ],
      "id": "55c136fe-0e89-4d39-a627-04fc364bf146"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"Recommend a name for a company that makes {what}\""
      ],
      "id": "0bd25659"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets set that up and call it with `what=\"cars\"`:"
      ],
      "id": "6d30f3b6-df05-4a1a-826d-54471ff8d585"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt=template.format(what=\"cars\")\n",
        "response = get_completion(prompt)\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"Response:\")\n",
        "print(response)"
      ],
      "id": "99e15f39"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Change the template variable `{what}`\n",
        "\n",
        "Letâ€™s call the LLM using `what=\"books\"` as the template variable."
      ],
      "id": "17588445-9b91-407a-922d-cb37777df62b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt=template.format(what=\"books\")\n",
        "response = get_completion(prompt)\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"Response:\")\n",
        "print(response)"
      ],
      "id": "e05b70d3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add A Template Variable `{num}`"
      ],
      "id": "43ab314c-3c00-4203-a80f-1a20f6483bf1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"Recommend {num} names for a company that makes {what}\""
      ],
      "id": "5bdf1273"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets call it with `num=3` and `what=\"cars\"`:"
      ],
      "id": "8575e0f7-2635-4176-9167-4563afaf3889"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt=template.format(num=3, what=\"cars\")\n",
        "response = get_completion(prompt)\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"Response:\")\n",
        "print(response)"
      ],
      "id": "23d543e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add a Template Variable `{format}`\n",
        "\n",
        "Lets see if we can introduce some formatting to the output:"
      ],
      "id": "9ea97e8c-24d6-409f-aaac-99ee19780934"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template =\"\"\"Recommend {num} names for a company that makes {what}.\n",
        "Give your answer in {format}\"\"\""
      ],
      "id": "ed193810"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Call the LLM:"
      ],
      "id": "28167dd4-5123-491e-a751-9d14fbb7673d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt=template.format(num=3, what=\"cars\", format=\"A bulleted list using upper case\")\n",
        "response = get_completion(prompt)\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"Response:\")\n",
        "print(response)"
      ],
      "id": "b4655489"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    }
  }
}