{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLMS\n",
        "\n",
        "## Set up your `OPENAI_API_KEY`"
      ],
      "id": "48bbb02f-35c3-4468-b581-1a1bc22b2cf6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.environ.get(\"OPENAI_API_KEY\") is None:\n",
        "   os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" #set your api key here."
      ],
      "id": "5793ffeb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Packages to Call the LLM\n",
        "\n",
        "For this part we set up the LLM."
      ],
      "id": "e7d2ab69-5380-4ae2-a820-2b4a952edde6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install langchain\n",
        "!pip3 install langchain-openai\n",
        "!pip3 install openai"
      ],
      "id": "14b28d48"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "import textwrap\n",
        "\n",
        "def wrap_text(text, max_width=80):\n",
        "    \"\"\"\n",
        "    Wraps the text to the specified max_width, preserving line breaks and formatting.\n",
        "    \"\"\"\n",
        "    lines = text.splitlines()  # Split the text into lines\n",
        "    wrapped_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.strip():  # Skip empty lines\n",
        "            wrapped_line = textwrap.fill(line, max_width, initial_indent='', subsequent_indent='')\n",
        "            wrapped_lines.extend(wrapped_line.splitlines())  # Preserve line breaks\n",
        "        else:\n",
        "            wrapped_lines.append('')  # Keep empty lines\n",
        "\n",
        "    return '\\n'.join(wrapped_lines)\n",
        "\n",
        "\n",
        "def print_prompt_and_response(prompt, response):\n",
        "    print(\"Prompt: \")\n",
        "    print(prompt)\n",
        "    print(\"\")\n",
        "    print(\"Response: \")\n",
        "    print(response)\n",
        "    print(80*\"=\")\n",
        "\n",
        "def get_completion(prompt, temperature=0.0):\n",
        "    # Create the OpenAI LLM\n",
        "    llm = ChatOpenAI(\n",
        "               model_name =\"gpt-3.5-turbo\",\n",
        "               temperature=temperature\n",
        "             )\n",
        "\n",
        "    # Call the LLM with the prompt\n",
        "    response = llm.invoke(prompt)\n",
        "    wrapped_response = wrap_text(response.content)\n",
        "    return wrapped_response"
      ],
      "id": "62f0a3f4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling the LLM using a template\n",
        "\n",
        "Remember the template for having the LLM give us the name of a company\n",
        "that makes a particular thing? Let’s call the LLM with that template and\n",
        "template variable `Cars`:"
      ],
      "id": "8d769173-7045-4839-b142-6e6ef6a40f09"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"Create a name for a company that makes {what}\"\n",
        "\n",
        "#Evaluate the template with what=\"Cars\"\n",
        "prompt=template.format(what=\"Cars\")\n",
        "\n",
        "#Call the LLM\n",
        "response = get_completion(prompt)\n",
        "\n",
        "#Print out the the response\n",
        "print(response)"
      ],
      "id": "97032d06"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Change the template variable `what`\n",
        "\n",
        "Let’s call the LLM using `Books` as the template variable."
      ],
      "id": "73003572-a701-4725-b904-74093354bcf7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Evaluate the template with what=\"Books\"\n",
        "prompt=template.format(what=\"Books\")\n",
        "response = get_completion(prompt)\n",
        "\n",
        "print(response)"
      ],
      "id": "c4e0a416"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add another template variable for `num`"
      ],
      "id": "fe04ddf6-a211-415a-bf92-63f399fd42a9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"Give {num} possible names for a company that makes {what}\"\n",
        "\n",
        "#Evaluate the template\n",
        "prompt=template.format(num=3, what=\"Cars\")\n",
        "response = get_completion(prompt)\n",
        "\n",
        "print(response)"
      ],
      "id": "81962cf9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Formatting the response from the LLM\n",
        "\n",
        "Lets see if we can introduce some formatting to the output:"
      ],
      "id": "28a5754d-6272-42c9-9263-2b8819f65cd7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template =\"\"\"Give {num} possible names for a company that makes {what}. Give\n",
        "your answer in {format}\"\"\"\n",
        "\n",
        "#Evaluate the template\n",
        "prompt=template.format(num=3, what=\"Cars\", format=\"A bulleted list using upper case\")\n",
        "response = get_completion(prompt)\n",
        "\n",
        "print(response)"
      ],
      "id": "7f48e8e7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling the LLM multiple times with our template\n",
        "\n",
        "The following is an example of calling the LLM multiple times, each time\n",
        "with a different prompt that is created from our template."
      ],
      "id": "472b9c02-24e6-41db-8ebe-f0aa259b62b8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template =\"\"\"Give {num} possible names for a company that makes {what}. Give\n",
        "your answer in {format}\"\"\"\n",
        "\n",
        "myList = [\"cars\", \"watches\", \"firecrackers\"]\n",
        "\n",
        "for item in myList:\n",
        "  prompt = template.format(num=2, what=item, format=\"upper case\")\n",
        "  response = get_completion(prompt)\n",
        "  print_prompt_and_response(prompt, response)"
      ],
      "id": "9ecf4963"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    }
  }
}