{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLMS\n",
        "\n",
        "## Set up your `OPENAI_API_KEY`"
      ],
      "id": "507de719-9493-4045-b824-b4a313c8a473"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.environ.get(\"OPENAI_API_KEY\") is None:\n",
        "   os.environ[\"OPENAI_API_KEY\"] = \"paste your api key here\""
      ],
      "id": "5d6dedf7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Packages to Call the LLM\n",
        "\n",
        "For this part we set up the LLM."
      ],
      "id": "8e92de2a-0ea2-4c42-9cbc-3cddedf17cac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install openai"
      ],
      "id": "81bdf8cb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import textwrap\n",
        "\n",
        "def wrap_text(text, max_width=80):\n",
        "    \"\"\"\n",
        "    Wraps the text to the specified max_width, preserving line breaks and formatting.\n",
        "    \"\"\"\n",
        "    lines = text.splitlines()  # Split the text into lines\n",
        "    wrapped_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.strip():  # Skip empty lines\n",
        "            wrapped_line = textwrap.fill(line, max_width, initial_indent='', subsequent_indent='')\n",
        "            wrapped_lines.extend(wrapped_line.splitlines())  # Preserve line breaks\n",
        "        else:\n",
        "            wrapped_lines.append('')  # Keep empty lines\n",
        "\n",
        "    return '\\n'.join(wrapped_lines)\n",
        "\n",
        "\n",
        "def print_prompt_and_response(prompt, response):\n",
        "    print(\"Prompt: \")\n",
        "    print(prompt)\n",
        "    print(\"\")\n",
        "    print(\"Response: \")\n",
        "    print(response)\n",
        "    print(80*\"=\")\n",
        "\n",
        "def get_completion(prompt, temperature=0.0):\n",
        "    # Create the OpenAI LLM\n",
        "    client = openai.OpenAI()\n",
        "    chat_completion = client.chat.completions.create(\n",
        "                           model=\"gpt-3.5-turbo\",\n",
        "                           messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                           temperature=temperature,\n",
        "                           max_tokens=1024)\n",
        "\n",
        "    response = chat_completion.choices[0].message.content\n",
        "    wrapped_response = wrap_text(response)\n",
        "    return wrapped_response"
      ],
      "id": "1b4f1a30"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling an LLM\n",
        "\n",
        "We can call the LLM by using the `get_completion` function. This takes\n",
        "our prompt and delivers it to the LLM. The LLM returns a response. We\n",
        "can print the response after we get it."
      ],
      "id": "945b84cb-563f-434c-87ea-a03cb565c21f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Set up a prompt\n",
        "prompt = \"Why is the sky blue? In 50 words or less.\"\n",
        "\n",
        "#Call the LLM\n",
        "response = get_completion(prompt)\n",
        "\n",
        "#Print out the the response\n",
        "print(response)"
      ],
      "id": "10805d70"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling an LLM with the `temperature`\n",
        "\n",
        "Temperature is one of the settings that most LLMs have. You can set the\n",
        "temperature from 0.0 up to some maximum (like 1.0). Sometimes you can go\n",
        "higher than 1.0.\n",
        "\n",
        "Roughly\n",
        "\n",
        "1.  Low temperature (eg 0.2-0.5)\n",
        "    -   Generates more predictable and conservative text\n",
        "2.  Medium temperature (eg. 0.6-0.8)\n",
        "    -   Balances between creativity and coherence\n",
        "3.  High temperature (eg. 0.9-1.2)\n",
        "    -   Generates more creative diverse, and unpredictable text\n",
        "\n",
        "### Calling with `temperature=0.0`\n",
        "\n",
        "Here we call the LLM 3 times with temperature 0.0"
      ],
      "id": "c41a8ad1-da7a-4500-bcbc-b597f630b453"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Why is the sky blue? In 50 words or less\"\n",
        "response = get_completion(prompt, temperature = 0.0)\n",
        "print(response)"
      ],
      "id": "a0cb2db1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt, temperature = 0.0)\n",
        "print(response)"
      ],
      "id": "d513480b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt, temperature = 0.0)\n",
        "print(response)"
      ],
      "id": "87c323a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calling with `temperature=0.4`\n",
        "\n",
        "Here we call the LLM 3 times with temperature 0.4"
      ],
      "id": "b11c16ba-8838-4055-8b45-88c7bd367799"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt, temperature = 0.4)\n",
        "print(response)"
      ],
      "id": "118a4f1b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt, temperature = 0.4)\n",
        "print(response)"
      ],
      "id": "ab4fddd2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt, temperature = 0.4)\n",
        "print(response)"
      ],
      "id": "8cad2ac2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calling with `temperature=1.0`\n",
        "\n",
        "Here we call the LLM 3 times with temperature 1.0"
      ],
      "id": "f4c90440-37c5-44e2-a954-e2e63a92672f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt, temperature = 1.0)\n",
        "print(response)"
      ],
      "id": "3833a202"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt, temperature = 1.0)\n",
        "print(response)"
      ],
      "id": "fc2cd142"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt, temperature = 1.0)\n",
        "print(response)"
      ],
      "id": "2bba4df2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling the LLM using a template\n",
        "\n",
        "Remember the template for having the LLM give us the name of a company\n",
        "that makes a particular thing? Let’s call the LLM with that template and\n",
        "template variable `Cars`:"
      ],
      "id": "7e3d7bfd-6060-40f3-84b5-727a3c125394"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"Create a name for a company that makes {what}\"\n",
        "\n",
        "#Evaluate the template with what=\"Cars\"\n",
        "prompt=template.format(what=\"Cars\")\n",
        "\n",
        "#Call the LLM\n",
        "response = get_completion(prompt)\n",
        "\n",
        "#Print out the the response\n",
        "print(response)"
      ],
      "id": "a766930c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Change the template variable `what`\n",
        "\n",
        "Let’s call the LLM using `Books` as the template variable."
      ],
      "id": "5398dd0c-9096-4626-b294-d486b04ee739"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Evaluate the template with what=\"Books\"\n",
        "prompt=template.format(what=\"Books\")\n",
        "response = get_completion(prompt)\n",
        "\n",
        "print(response)"
      ],
      "id": "42bd8698"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add another template variable for `num`"
      ],
      "id": "5a45a234-c7af-4423-972a-ad16d625b4f4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"Give {num} possible names for a company that makes {what}\"\n",
        "\n",
        "#Evaluate the template\n",
        "prompt=template.format(num=3, what=\"Cars\")\n",
        "response = get_completion(prompt)\n",
        "\n",
        "print(response)"
      ],
      "id": "57e932aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Formatting the response from the LLM\n",
        "\n",
        "Lets see if we can introduce some formatting to the output:"
      ],
      "id": "0023be0c-29a7-4b80-92d7-093c057d7c2b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template =\"\"\"Give {num} possible names for a company that makes {what}. Give\n",
        "your answer in {format}\"\"\"\n",
        "\n",
        "#Evaluate the template\n",
        "prompt=template.format(num=3, what=\"Cars\", format=\"A bulleted list using upper case\")\n",
        "response = get_completion(prompt)\n",
        "\n",
        "print(response)"
      ],
      "id": "63a01889"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling the LLM multiple times with our template\n",
        "\n",
        "The following is an example of calling the LLM multiple times, each time\n",
        "with a different prompt that is created from our template."
      ],
      "id": "1782c287-0b0d-4e27-8da0-64ff36e2644f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template =\"\"\"Give {num} possible names for a company that makes {what}. Give\n",
        "your answer in {format}\"\"\"\n",
        "\n",
        "myList = [\"cars\", \"watches\", \"firecrackers\"]\n",
        "\n",
        "for item in myList:\n",
        "  prompt = template.format(num=2, what=item, format=\"upper case\")\n",
        "  response = get_completion(prompt)\n",
        "  print_prompt_and_response(prompt, response)"
      ],
      "id": "5f8b4867"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    }
  }
}