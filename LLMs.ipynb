{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLMS"
      ],
      "id": "b37b164f-8552-4f78-9fc6-f4ccd773f202"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.environ.get(\"TOGETHER_API_KEY\") is None:\n",
        "   os.environ[\"TOGETHER_API_KEY\"] = \"paste_your_api_key_here\""
      ],
      "id": "68fbccd2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install langchain langchain_openai"
      ],
      "id": "1e91a70f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model_name = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT\"\n",
        "#model_name = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
        "#model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "#model_name=\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "#model_name=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "#model_name = \"openchat/openchat-3.5-1210\"\n",
        "#model_name = \"Qwen/Qwen1.5-72B-Chat\"\n",
        "\n",
        "print(\"Provider: TogetherAI\")\n",
        "print(\"Model: \" + model_name)\n",
        "\n",
        "llm=ChatOpenAI(model_name=model_name,\n",
        "               openai_api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
        "               openai_api_base=\"https://api.together.xyz/v1/\")"
      ],
      "id": "048bb9c9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import textwrap\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "def wrap_text(text, max_width=80):\n",
        "    \"\"\"\n",
        "    Wraps the text to the specified max_width, preserving line breaks and formatting.\n",
        "    \"\"\"\n",
        "    text = text.lstrip()\n",
        "    lines = text.splitlines()  # Split the text into lines\n",
        "    wrapped_lines = []\n",
        "    for line in lines:\n",
        "        if line.strip():  # Skip empty lines\n",
        "            wrapped_line = textwrap.fill(line, max_width, initial_indent='', subsequent_indent='')\n",
        "            wrapped_lines.extend(wrapped_line.splitlines())  # Preserve line breaks\n",
        "        else:\n",
        "            wrapped_lines.append('')  # Keep empty lines\n",
        "    return '\\n'.join(wrapped_lines)\n",
        "\n",
        "def print_prompt_and_response(prompt, response):\n",
        "    print(\"Prompt: \")\n",
        "    print(wrap_text(prompt))\n",
        "    print(\"\")\n",
        "    print(\"Response: \")\n",
        "    print(response)\n",
        "\n",
        "def print_messages_and_response(messages, response):\n",
        "    prompt = ChatPromptTemplate(messages=messages)\n",
        "    print_prompt_and_response(prompt.format(), response)\n",
        "\n",
        "def get_completion(prompt, temperature=0.0):\n",
        "    response = llm.invoke(prompt, temperature=temperature)\n",
        "    wrapped_response = wrap_text(response.content)\n",
        "    return wrapped_response\n",
        "\n",
        "def get_completion_messages(messages, temperature=0.0):\n",
        "    response=llm.invoke(messages, temperature=temperature)\n",
        "    wrapped_response = wrap_text(response.content)\n",
        "    return wrapped_response"
      ],
      "id": "6579087f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling an LLM with `get_completion`\n",
        "\n",
        "First set up a prompt we can use:"
      ],
      "id": "3ef0b4b3-d6ba-4ef6-b5bc-2ab7f2d19f98"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Why is the sky blue? In 50 words or less.\""
      ],
      "id": "60c1308d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we call the`get_completion` function. This function takes our prompt\n",
        "as argument and sends it off to the LLM. The LLM returns a response.\n",
        "This function returns that response."
      ],
      "id": "352e822e-e2a5-4339-a3ae-4196686bca08"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "id": "e41cec55"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sometimes its nice to print out the prompt and the response when we are\n",
        "working like this. We can use the function `print_prompt_and_response`\n",
        "and pass it the prompt and the response we got from the `get_completion`\n",
        "function."
      ],
      "id": "210dd784-2ffd-4167-90f1-9bcee7ccc9d4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt)\n",
        "print_prompt_and_response(prompt, response)"
      ],
      "id": "5bc84b77"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling an LLM with the `temperature`\n",
        "\n",
        "Temperature is one of the settings that most LLMs have. You can set the\n",
        "temperature from 0.0 up to some maximum (like 1.0 or 1.2). Sometimes you\n",
        "can go higher.\n",
        "\n",
        "Roughly\n",
        "\n",
        "1.  Low temperature (eg 0.2-0.5)\n",
        "    -   Generates more predictable and conservative text\n",
        "2.  Medium temperature (eg. 0.6-0.8)\n",
        "    -   Balances between creativity and coherence\n",
        "3.  High temperature (eg. 0.9-1.2)\n",
        "    -   Generates more creative diverse, and unpredictable text\n",
        "\n",
        "First lets set up a prompt to call several times with different\n",
        "temperatures."
      ],
      "id": "df25611e-4a9f-45ed-802e-497db52315ec"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Why is the sky blue? In 50 words or less\""
      ],
      "id": "d53a424a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calling with `temperature=0.0`\n",
        "\n",
        "`temperature` is parameter you can pass to the `get_completion`\n",
        "function. You see this in the code below.\n",
        "\n",
        "Here we call the LLM 3 times with temperature 0.0"
      ],
      "id": "85cd8313-fc92-4141-b54c-6f8647f10b7c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt, temperature = 0.0)\n",
        "print(response)\n",
        "print(\"------\")\n",
        "\n",
        "response = get_completion(prompt, temperature = 0.0)\n",
        "print(response)\n",
        "print(\"------\")\n",
        "\n",
        "response = get_completion(prompt, temperature = 0.0)\n",
        "print(response)\n",
        "print(\"------\")"
      ],
      "id": "15e23796"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calling with `temperature=0.4`\n",
        "\n",
        "Here we call the LLM 3 times with temperature 0.4"
      ],
      "id": "141bde73-bd95-4e77-914c-0329f5a92e7f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt, temperature = 0.4)\n",
        "print(response)\n",
        "print(\"------\")\n",
        "\n",
        "response = get_completion(prompt, temperature = 0.4)\n",
        "print(response)\n",
        "print(\"------\")\n",
        "\n",
        "response = get_completion(prompt, temperature = 0.4)\n",
        "print(response)\n",
        "print(\"------\")"
      ],
      "id": "71a93cd0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calling with `temperature=1.0`"
      ],
      "id": "3007829d-9293-4b7b-9356-b2a8e132e06f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = get_completion(prompt, temperature = 1.0)\n",
        "print(response)\n",
        "print(\"------\")\n",
        "\n",
        "response = get_completion(prompt, temperature = 1.0)\n",
        "print(response)\n",
        "print(\"------\")\n",
        "\n",
        "response = get_completion(prompt, temperature = 1.0)\n",
        "print(response)\n",
        "print(\"------\")"
      ],
      "id": "891def1a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling the LLM using a template\n",
        "\n",
        "Remember the template for having the LLM give us the name of a company\n",
        "that makes a particular thing?"
      ],
      "id": "9b136f41-458f-4478-9153-5dae55852081"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"Recommend a name for a company that makes {what}\""
      ],
      "id": "af9dde64"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets set that up and call it with `what=\"cars\"`:"
      ],
      "id": "11eeed7e-855b-406d-9d96-0161bd85ed90"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt=template.format(what=\"cars\")\n",
        "response = get_completion(prompt)\n",
        "print_prompt_and_response(prompt, response)"
      ],
      "id": "5ecf81ed"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Change the template variable `{what}`\n",
        "\n",
        "Let’s call the LLM using `what=\"books\"` as the template variable."
      ],
      "id": "ca6e25dc-00b1-42a2-83e4-71659d58c30b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt=template.format(what=\"books\")\n",
        "response = get_completion(prompt)\n",
        "print_prompt_and_response(prompt, response)"
      ],
      "id": "436dd80c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add A Template Variable `{num}`"
      ],
      "id": "d2d6e1d8-0b08-4813-bd6f-dbdc43e6595f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"Recommend {num} names for a company that makes {what}\""
      ],
      "id": "0a330bfd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets call it with `num=3` and `what=\"cars\"`:"
      ],
      "id": "ed197f24-3b67-4a8e-93e1-dcba0feb9fe5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt=template.format(num=3, what=\"cars\")\n",
        "response = get_completion(prompt)\n",
        "print_prompt_and_response(prompt, response)"
      ],
      "id": "4a99fa1c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add a Template Variable `{format}`\n",
        "\n",
        "Lets see if we can introduce some formatting to the output:"
      ],
      "id": "4396128e-6151-49ce-900f-23affd789709"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template =\"\"\"Recommend {num} names for a company that makes {what}.\n",
        "Give your answer in {format}\"\"\""
      ],
      "id": "73c1aeaa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Call the LLM:"
      ],
      "id": "d3f610f2-4d69-4b8a-b76e-39cc7d1e7552"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt=template.format(num=3, what=\"cars\", format=\"A bulleted list using upper case\")\n",
        "response = get_completion(prompt)\n",
        "print_prompt_and_response(prompt, response)"
      ],
      "id": "cedbc940"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    }
  }
}